**课程地址：**<br/>
https://www.bilibili.com/video/BV164411m79z?from=search&seid=3072377659384001779 <br/>
https://www.bilibili.com/video/BV1V441127zE?from=search&seid=3072377659384001779 <br/>
https://www.bilibili.com/video/BV1f4411C7Nx?from=search&seid=3072377659384001779 <br/>
https://www.bilibili.com/video/BV1F4411y7o7?from=search&seid=3072377659384001779 <br/>
https://www.bilibili.com/video/BV1F4411y7BA?from=search&seid=3072377659384001779 <br/>

# （一）《神经网络和深度学习》（4周）

## 第一周  深度学习概论：

学习驱动神经网络兴起的主要技术趋势，了解现今深度学习在哪里应用、如何应用。<br/>

1.1  欢迎来到深度学习工程师微专业<br/>
1.2  什么是神经网络？<br/>
1.3  用神经网络进行监督学习<br/>
1.4  为什么深度学习会兴起？<br/>
1.5  关于这门课<br/>
1.6  课程资源<br/>

 
## 第二周  神经网络基础：

学习如何用神经网络的思维模式提出机器学习问题、如何使用向量化加速你的模型。<br/>

2.1  二分分类<br/>
2.2  logistic 回归<br/>
2.3  logistic 回归损失函数<br/>
2.4  梯度下降法<br/>
2.5  导数<br/>
2.6  更多导数的例子<br/>
2.7  计算图<br/>
2.8  计算图的导数计算<br/>
2.9  logistic 回归中的梯度下降法<br/>
2.10  m 个样本的梯度下降<br/>
2.11  向量化<br/>
2.12  向量化的更多例子<br/>
2.13  向量化 logistic 回归<br/>
2.14  向量化 logistic 回归的梯度输出<br/>
2.15  Python 中的广播<br/>
2.16  关于 python / numpy 向量的说明<br/>
2.17  Jupyter / Ipython 笔记本的快速指南<br/>
2.18  （选修）logistic 损失函数的解释<br/>

 
## 第三周  浅层神经网络：

学习使用前向传播和反向传播搭建出有一个隐藏层的神经网络。<br/>

3.1  神经网络概览<br/>
3.2  神经网络表示<br/>
3.3  计算神经网络的输出<br/>
3.4  多样本向量化<br/>
3.5  向量化实现的解释<br/>
3.6  激活函数<br/>
3.7  为什么需要非线性激活函数？<br/>
3.8  激活函数的导数<br/>
3.9  神经网络的梯度下降法<br/>
3.10  （选修）直观理解反向传播<br/>
3.11  随机初始化<br/>

 
## 第四周  深层神经网络：

理解深度学习中的关键计算，使用它们搭建并训练深层神经网络，并应用在计算机视觉中。<br/>

4.1  深层神经网络<br/>
4.2  深层网络中的前向传播<br/>
4.3  核对矩阵的维数<br/>
4.4  为什么使用深层表示<br/>
4.5  搭建深层神经网络块<br/>
4.6  前向和反向传播<br/>
4.7  参数 VS 超参数<br/>
4.8  这和大脑有什么关系？<br/>


# （二）《改善深层神经网络：超参数调试、正则化以及优化》（三周）
## 第一周  深度学习的实用层面：

1.1  训练/开发/测试集<br/>
1.2  偏差/方差<br/>
1.3  机器学习基础<br/>
1.4  正则化<br/>
1.5  为什么正则化可以减少过拟合？<br/>
1.6  Dropout 正则化<br/>
1.7  理解 Dropout<br/>
1.8  其他正则化方法<br/>
1.9  正则化输入<br/>
1.10  梯度消失与梯度爆炸<br/>
1.11  神经网络的权重初始化<br/>
1.12  梯度的数值逼近<br/>
1.13  梯度检验<br/>
1.14  关于梯度检验实现的注记<br/>


## 第二周  优化算法：

2.1  Mini-batch 梯度下降法<br/>
2.2  理解 mini-batch 梯度下降法<br/>
2.3  指数加权平均<br/>
2.4  理解指数加权平均<br/>
2.5  指数加权平均的偏差修正<br/>
2.6  动量梯度下降法<br/>
2.7  RMSprop<br/>
2.8  Adam 优化算法<br/>
2.9  学习率衰减<br/>
2.10  局部最优的问题<br/>


## 第三周  超参数调试、Batch正则化和程序框架

3.1  调试处理<br/>
3.2  为超参数选择合适的范围<br/>
3.3  超参数训练的实践：Pandas VS Caviar<br/>
3.4  正则化网络的激活函数<br/>
3.5  将 Batch Norm 拟合进神经网络<br/>
3.6  Batch Norm 为什么奏效？<br/>
3.7  测试时的 Batch Norm<br/>
3.8  Softmax 回归<br/>
3.9  训练一个 Softmax 分类器<br/>
3.10  深度学习框架<br/>
3.11  TensorFlow<br/>

# （三）《结构化机器学习项目》（两周）
## 第一周  机器学习（ML）策略（1）

1.1  为什么是ML策略<br/>
1.2  正交化<br/>
1.3  单一数字评估指标<br/>
1.4  满足和优化指标<br/>
1.5  训练/开发/测试集划分<br/>
1.6  开发集合测试集的大小<br/>
1.7  什么时候该改变开发/测试集和指标<br/>
1.8  为什么是人的表现<br/>
1.9  可避免偏差<br/>
1.10  理解人的表现<br/>
1.11  超过人的表现<br/>
1.12  改善你的模型的表现<br/>


## 第二周  机器学习（ML）策略（2）

2.1  进行误差分析<br/>
2.2  清楚标注错误的数据<br/>
2.3  快速搭建你的第一个系统，并进行迭代<br/>
2.4  在不同的划分上进行训练并测试<br/>
2.5  不匹配数据划分的偏差和方差<br/>
2.6  定位数据不匹配<br/>
2.7  迁移学习<br/>
2.8  多任务学习<br/>
2.9  什么是端到端的深度学习<br/>
2.10 是否要使用端到端的深度学习<br/>

# （四）《卷积神经网络》（四周）
## 第一周  卷积神经网络

1.1  计算机视觉<br/>
1.2  边缘检测示例<br/>
1.3  更多边缘检测内容<br/>
1.4  Padding<br/>
1.5  卷积步长<br/>
1.6  卷积的“卷”体现之处<br/>
1.7  单层卷积网络<br/>
1.8  简单卷积网络示例<br/>
1.9  池化层<br/>
1.10  卷积神经网络示例<br/>
1.11  为什么使用卷积？<br/>

## 第二周  深度卷积网络：实例探究

2.1  为什么要进行实例探究<br/>
2.2  经典网络<br/>
2.3  残差网络<br/>
2.4  残差网络为什么有用？<br/>
2.5  网络中的网络以及 1×1 卷积<br/>
2.6  谷歌 Inception 网络简介<br/>
2.7  Inception 网络<br/>
2.8  使用开源的实现方案<br/>
2.9  迁移学习<br/>
2.10  数据扩充<br/>
2.11  计算机视觉现状<br/>

## 第三周  目标检测

3.1  目标定位<br/>
3.2  特征点检测<br/>
3.3  目标检测<br/>
3.4  卷积的滑动窗口实现<br/>
3.5  Bounding Box预测<br/>
3.6  交并比<br/>
3.7  非极大值抑制<br/>
3.8  Anchor Boxes<br/>
3.9  YOLO 算法<br/>
3.10  RPN网络<br/>

## 第四周  特殊应用：人脸识别和神经风格转换

4.1  什么是人脸识别？<br/>
4.2  One-Shot 学习<br/>
4.3  Siamese 网络<br/>
4.4  Triplet 损失<br/>
4.5  面部验证与二分类<br/>
4.6  什么是神经风格转换？<br/>
4.7  什么是深度卷积网络？<br/>
4.8  代价函数<br/>
4.9  内容代价函数<br/>
4.10  风格代价函数<br/>
4.11 一维到三维推广<br/>

# （五）《序列模型》
## 第一周  循环序列模型

本周的知识点是循环神经网络。这种类型的模型已经被证明在时间数据上表现非常好，它有几个变体，包括 LSTM、GRU 和双向神经网络，本周的课程中也都包括这些内容。<br/>

1.1  为什么选择序列模型<br/>
1.2  数学符号<br/>
1.3  循环神经网络模型<br/>
1.4  通过时间的反向传播<br/>
1.5  不同类型的循环神经网络<br/>
1.6  语言模型和序列生成<br/>
1.7  对新序列采样<br/>
1.8  带有神经网络的梯度消失<br/>
1.9  GRU 单元<br/>
1.10  长短期记忆（LSTM）<br/>
1.11  双向神经网络<br/>
1.12  深层循环神经网络<br/>


## 第二周  自然语言处理与词嵌入

自然语言处理与深度学习是特别重要的组合。使用词向量表示和嵌入层，可以训练在各种行业中表现出色的循环神经网络。应用程序示例包括情绪分析、物体识别和机器翻译。<br/>

2.1  词汇表征<br/>
2.2  使用词嵌入<br/>
2.3  词嵌入的特性<br/>
2.4  嵌入矩阵<br/>
2.5  学习词嵌入<br/>
2.6  Word2Vec<br/>
2.7  负采样<br/>
2.8  GloVe 词向量<br/>
2.9  情绪分类<br/>
2.10  词嵌入除偏<br/>


## 第三周  序列模型和注意力机制

注意力机制可以增强序列模型。这个算法将帮助你的模型理解，在给出一系列的输入时，它应该把注意力放在什么地方。本周，你还将学习语音识别以及如何处理音频数据。<br/>

3.1  基础模型<br/>
3.2  选择最可能的句子<br/>
3.3  定向搜索<br/>
3.4  改进定向搜索<br/>
3.5  定向搜索的误差分析<br/>
3.6  Bleu 得分（选修）<br/>
3.7  注意力模型直观理解<br/>
3.8  注意力模型<br/>
3.9  语音辨识<br/>
3.10  触发字检测<br/>
3.11  结论和致谢<br/>
